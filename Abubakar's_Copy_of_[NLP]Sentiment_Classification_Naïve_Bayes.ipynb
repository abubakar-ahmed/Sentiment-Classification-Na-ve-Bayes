{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "import kagglehub\n",
        "avikumart_movies_reviews_dataset_path = kagglehub.dataset_download('avikumart/movies-reviews-dataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "An9UO9SFKaxR"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "**In today's world, one of the biggest sources of information is text data, which is unstructured in nature, finding customer sentiments from product reviews or feedback, extracting opinions from social media data are a few examples of text analytics.**\n",
        "\n",
        "**Finding Insights from text data is not as straight forward as structured data and it need extensive data pre-precessing. pre-precessing inclues removing punctuations, removing stop-words and cleaning words to it root format so that vectorized data contains meaningful features that maps target variable well enough.**\n",
        "\n",
        "**In this notebook, we will use `Count vectorizer` & `TF-IDF vectorizer` along with stemming and n-grams to pre-precess and vectorize the movie reviews data before we build sentiment classification models for training and testing.**\n",
        "\n",
        "![image.png](attachment:f3dbf741-e35d-4e3d-95e9-a82a6b98ef72.png)"
      ],
      "metadata": {
        "id": "y9O9KQgiKaxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing libraries"
      ],
      "metadata": {
        "id": "X__5sPZUKaxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# standard data science library\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "%matplotlib inline\n",
        "\n",
        "# Text processing and model building libraries\n",
        "import random\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction import text\n",
        "from nltk.stem.snowball import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Models\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "# matplotlib defaults\n",
        "plt.style.use(\"seaborn-darkgrid\")\n",
        "plt.rc(\"figure\", autolayout=True)\n",
        "plt.rc(\n",
        "    \"axes\",\n",
        "    labelweight=\"bold\",\n",
        "    labelsize=\"large\",\n",
        "    titleweight=\"bold\",\n",
        "    titlesize=14,\n",
        "    titlepad=10,\n",
        ")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2022-07-04T10:24:17.485741Z",
          "iopub.execute_input": "2022-07-04T10:24:17.486386Z",
          "iopub.status.idle": "2022-07-04T10:24:20.064934Z",
          "shell.execute_reply.started": "2022-07-04T10:24:17.486243Z",
          "shell.execute_reply": "2022-07-04T10:24:20.063217Z"
        },
        "trusted": true,
        "id": "tDfTCKIdKaxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and explore a dataset"
      ],
      "metadata": {
        "id": "V5dp3A8WKaxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = pd.read_csv(\"/kaggle/input/movies-reviews-dataset/sentiment_train\", delimiter=\"\\t\")\n",
        "train_ds.head(10)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:24:20.067855Z",
          "iopub.execute_input": "2022-07-04T10:24:20.06884Z",
          "iopub.status.idle": "2022-07-04T10:24:20.129653Z",
          "shell.execute_reply.started": "2022-07-04T10:24:20.068779Z",
          "shell.execute_reply": "2022-07-04T10:24:20.128278Z"
        },
        "trusted": true,
        "id": "V89lgm-FKaxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:24:20.131598Z",
          "iopub.execute_input": "2022-07-04T10:24:20.132677Z",
          "iopub.status.idle": "2022-07-04T10:24:20.142093Z",
          "shell.execute_reply.started": "2022-07-04T10:24:20.132613Z",
          "shell.execute_reply": "2022-07-04T10:24:20.140518Z"
        },
        "trusted": true,
        "id": "thGC5IrHKaxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:24:20.145113Z",
          "iopub.execute_input": "2022-07-04T10:24:20.147212Z",
          "iopub.status.idle": "2022-07-04T10:24:20.183822Z",
          "shell.execute_reply.started": "2022-07-04T10:24:20.147147Z",
          "shell.execute_reply": "2022-07-04T10:24:20.181133Z"
        },
        "trusted": true,
        "id": "6OobDgG3KaxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds.describe(include=['object','int64'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:24:20.186582Z",
          "iopub.execute_input": "2022-07-04T10:24:20.188773Z",
          "iopub.status.idle": "2022-07-04T10:24:20.236448Z",
          "shell.execute_reply.started": "2022-07-04T10:24:20.188685Z",
          "shell.execute_reply": "2022-07-04T10:24:20.234045Z"
        },
        "trusted": true,
        "id": "FWO8TIicKaxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight:** In our dataset there are 1291 unique reviews and most frquent one is 'I love Harry Potter'."
      ],
      "metadata": {
        "id": "qzIV5609KaxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set width of the columns as 800 charactors\n",
        "pd.set_option('max_colwidth', 800)\n",
        "train_ds[train_ds.sentiment == 1][:5] # five positive sentiments"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:24:20.239201Z",
          "iopub.execute_input": "2022-07-04T10:24:20.240208Z",
          "iopub.status.idle": "2022-07-04T10:24:20.259055Z",
          "shell.execute_reply.started": "2022-07-04T10:24:20.240152Z",
          "shell.execute_reply": "2022-07-04T10:24:20.257166Z"
        },
        "trusted": true,
        "id": "i-10Q7mrKaxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# five negative sentiments\n",
        "train_ds[train_ds.sentiment == 0][:5]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:24:20.263057Z",
          "iopub.execute_input": "2022-07-04T10:24:20.264142Z",
          "iopub.status.idle": "2022-07-04T10:24:20.281985Z",
          "shell.execute_reply.started": "2022-07-04T10:24:20.264073Z",
          "shell.execute_reply": "2022-07-04T10:24:20.27957Z"
        },
        "trusted": true,
        "id": "wOUzn2HZKaxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  number of reviews per sentiment\n",
        "sen_df = train_ds['sentiment'].value_counts().reset_index()\n",
        "sen_df['% of reviews'] = sen_df['sentiment'].map(lambda x : (x/sen_df['sentiment'].sum())*100)\n",
        "sen_df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:24:20.284987Z",
          "iopub.execute_input": "2022-07-04T10:24:20.286088Z",
          "iopub.status.idle": "2022-07-04T10:24:20.311736Z",
          "shell.execute_reply.started": "2022-07-04T10:24:20.286022Z",
          "shell.execute_reply": "2022-07-04T10:24:20.309191Z"
        },
        "trusted": true,
        "id": "2agvI7oeKaxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight:**\n",
        "\n",
        "**Approx 60% of reviews are positive while 43% of them are negative ones.**"
      ],
      "metadata": {
        "id": "Vn8bExJCKaxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the sentiments using count plot of the seaborn\n",
        "plt.figure(figsize=(6,6))\n",
        "ax = sns.countplot(x='sentiment', data=train_ds)\n",
        "for p in ax.patches:\n",
        "    ax.annotate(p.get_height(), (p.get_x()+0.1, p.get_height() + 50))\n",
        "plt.title(\"Positive and negative sentiments\")\n",
        "plt.xlabel(\"Sentiment\")\n",
        "plt.ylabel(\"Count of sentiment\")\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:24:20.314283Z",
          "iopub.execute_input": "2022-07-04T10:24:20.315503Z",
          "iopub.status.idle": "2022-07-04T10:24:20.648835Z",
          "shell.execute_reply.started": "2022-07-04T10:24:20.315432Z",
          "shell.execute_reply": "2022-07-04T10:24:20.647082Z"
        },
        "trusted": true,
        "id": "edSJZ9VFKaxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Pre-processing and vectorization of text data"
      ],
      "metadata": {
        "id": "xIhDD-5rKaxY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BOW (Bag of words model):**\n",
        "\n",
        "1. Count Vector model\n",
        "2. Term Frequecny Vector model\n",
        "3. Term-Frequency-Inverse Document Frequency (TF-IDF) Model\n",
        "\n",
        "In out dataset, each text line will be called as a document, which will be used as vector to calculate count vecotrizer and TF/TF-IDF vectors."
      ],
      "metadata": {
        "id": "11AUe3bJKaxY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Using Count vectorizer"
      ],
      "metadata": {
        "id": "VaoBnqyUKaxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the count vectorizer\n",
        "count_vectorizer = CountVectorizer()\n",
        "# Create the dictionary from the corpus\n",
        "feature_vector = count_vectorizer.fit(train_ds.text)\n",
        "# get the feature names\n",
        "features0 = feature_vector.get_feature_names()\n",
        "print(\"Total number of features: \", len(features0))\n",
        "print(\"-----------------------------------------\")\n",
        "# name of the each features\n",
        "print(feature_vector.get_feature_names_out())\n",
        "print(\"-----------------------------------------\")\n",
        "# random feature names\n",
        "print(random.sample(features0, 10))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:24:20.658662Z",
          "iopub.execute_input": "2022-07-04T10:24:20.659952Z",
          "iopub.status.idle": "2022-07-04T10:24:20.889306Z",
          "shell.execute_reply.started": "2022-07-04T10:24:20.659863Z",
          "shell.execute_reply": "2022-07-04T10:24:20.887419Z"
        },
        "trusted": true,
        "id": "duWb-H8wKaxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transform features to sparse matrix"
      ],
      "metadata": {
        "id": "Sm6lY1YVKaxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tranform the data frame to a vectorizer\n",
        "train_ds_features = count_vectorizer.transform(train_ds.text)\n",
        "print(type(train_ds_features))\n",
        "print(\"-----------------------------------------\")\n",
        "# shape of transoformed array\n",
        "print(train_ds_features.shape)\n",
        "print(\"-----------------------------------------\")\n",
        "# how sparse is this out matrix, find the non-zero values in matrix\n",
        "print(\"Non-zero values in our matrix:\", train_ds_features.getnnz())\n",
        "print(\"-----------------------------------------\")\n",
        "# percentage of total values set as a zeros\n",
        "print(\"Density of the matrix: \", train_ds_features.getnnz()*100/(train_ds_features.shape[0]*train_ds_features.shape[1]))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:24:20.89253Z",
          "iopub.execute_input": "2022-07-04T10:24:20.893233Z",
          "iopub.status.idle": "2022-07-04T10:24:21.02013Z",
          "shell.execute_reply.started": "2022-07-04T10:24:20.893171Z",
          "shell.execute_reply": "2022-07-04T10:24:21.019152Z"
        },
        "trusted": true,
        "id": "1Ko9x3NmKaxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert sparse matrix to dense pandas dataframe"
      ],
      "metadata": {
        "id": "cAZiFTYcKaxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# converting to dense dataframe\n",
        "train_ds_df = pd.DataFrame(train_ds_features.todense())\n",
        "train_ds_df.columns = features0\n",
        "# let's croos-check with original dataframe with our vectorised dataframe\n",
        "train_ds[0:1]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:24:21.021525Z",
          "iopub.execute_input": "2022-07-04T10:24:21.022205Z",
          "iopub.status.idle": "2022-07-04T10:24:21.096558Z",
          "shell.execute_reply.started": "2022-07-04T10:24:21.022167Z",
          "shell.execute_reply": "2022-07-04T10:24:21.094408Z"
        },
        "trusted": true,
        "id": "jlLesSwPKaxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first documents 150 to 157 features has 'awesome' word as a feature and it can be seen counted as 1.\n",
        "train_ds_df.iloc[:1, 150:157]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:24:21.098162Z",
          "iopub.execute_input": "2022-07-04T10:24:21.099208Z",
          "iopub.status.idle": "2022-07-04T10:24:21.115518Z",
          "shell.execute_reply.started": "2022-07-04T10:24:21.099156Z",
          "shell.execute_reply": "2022-07-04T10:24:21.113611Z"
        },
        "trusted": true,
        "id": "FCOxcircKaxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's also check words in our first document as its features in our dense dataframe\n",
        "train_ds_df[['the','da','vinci','code','book','is','just','awesome','something']][:1]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:24:21.117858Z",
          "iopub.execute_input": "2022-07-04T10:24:21.118496Z",
          "iopub.status.idle": "2022-07-04T10:24:21.14609Z",
          "shell.execute_reply.started": "2022-07-04T10:24:21.118455Z",
          "shell.execute_reply": "2022-07-04T10:24:21.144977Z"
        },
        "trusted": true,
        "id": "RrblKbvPKaxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let''s find out which words are most occuring\n",
        "feature_counts = train_ds_df.sum().reset_index().sort_values(by=0, ascending=False)\n",
        "feature_counts.columns = ['Features', 'Counts']\n",
        "feature_counts.head(10)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:24:21.147693Z",
          "iopub.execute_input": "2022-07-04T10:24:21.148149Z",
          "iopub.status.idle": "2022-07-04T10:24:21.203954Z",
          "shell.execute_reply.started": "2022-07-04T10:24:21.148101Z",
          "shell.execute_reply": "2022-07-04T10:24:21.202574Z"
        },
        "trusted": true,
        "id": "TN2KIHb7KaxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the distribution of words count\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.hist(feature_counts.Counts, bins=50, range= (0,2000))\n",
        "plt.xlabel(\"Frequecny of the words\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.title(\"Distribution of frequecny of each words in corpus\")\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:24:21.205832Z",
          "iopub.execute_input": "2022-07-04T10:24:21.213877Z",
          "iopub.status.idle": "2022-07-04T10:24:21.668214Z",
          "shell.execute_reply.started": "2022-07-04T10:24:21.213787Z",
          "shell.execute_reply": "2022-07-04T10:24:21.66606Z"
        },
        "trusted": true,
        "id": "wIDly6jaKaxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the rare words, words that are appear only 1 time\n",
        "len(feature_counts[feature_counts.Counts == 1])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:24:21.670703Z",
          "iopub.execute_input": "2022-07-04T10:24:21.671674Z",
          "iopub.status.idle": "2022-07-04T10:24:21.683804Z",
          "shell.execute_reply.started": "2022-07-04T10:24:21.67162Z",
          "shell.execute_reply": "2022-07-04T10:24:21.682007Z"
        },
        "trusted": true,
        "id": "Ax1LlnNlKaxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight:**\n",
        "\n",
        "**Out of total 2132 words/features 1228 appears only 1 time, we can remove non-significant words**"
      ],
      "metadata": {
        "id": "bRX7ZBawKaxa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove-Stop Words"
      ],
      "metadata": {
        "id": "bG6A-8rRKaxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_stop_words = text.ENGLISH_STOP_WORDS.union(['harry','potter','code','vinci','da','harry','mountain',\n",
        "                                              'movie','movies'])\n",
        "my_stop_words1 = list(my_stop_words)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:24:21.685824Z",
          "iopub.execute_input": "2022-07-04T10:24:21.687638Z",
          "iopub.status.idle": "2022-07-04T10:24:21.700928Z",
          "shell.execute_reply.started": "2022-07-04T10:24:21.687445Z",
          "shell.execute_reply": "2022-07-04T10:24:21.699314Z"
        },
        "trusted": true,
        "id": "Nd-1wbSRKaxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting stop-wrod list in countvectorizer\n",
        "count_vectorizer1 = CountVectorizer(stop_words = my_stop_words1,\n",
        "                                   max_features = 1000)    #keeping only 1000 top words\n",
        "feature_vector1 = count_vectorizer1.fit(train_ds.text)\n",
        "train_ds_features1 = count_vectorizer1.transform(train_ds.text)\n",
        "features = feature_vector1.get_feature_names()\n",
        "features_counts1 = np.sum(train_ds_features1.toarray(), axis=0)\n",
        "feature_counts1 = pd.DataFrame(dict(features = features,\n",
        "                                    counts = features_counts1))\n",
        "feature_counts1.sort_values('counts', ascending=False)[:15]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:24:21.703272Z",
          "iopub.execute_input": "2022-07-04T10:24:21.704531Z",
          "iopub.status.idle": "2022-07-04T10:24:21.968743Z",
          "shell.execute_reply.started": "2022-07-04T10:24:21.704464Z",
          "shell.execute_reply": "2022-07-04T10:24:21.966917Z"
        },
        "trusted": true,
        "id": "-ju3FwIZKaxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming and lemmatization"
      ],
      "metadata": {
        "id": "rjDxHempKaxa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming: It chops-off the words to the stemmed words**\n",
        "\n",
        "**LemmatizationL It converts the words into its root form (i.e English Dictionary)**\n",
        "\n",
        "![image.png](attachment:d76b1387-d2b5-44c6-a6e7-7404a4fad990.png)"
      ],
      "metadata": {
        "id": "FpMgksv8Kaxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's learn what is stemming and lemmatization by example\n",
        "print(\"Original:\")\n",
        "print(train_ds.text[1])\n",
        "print()\n",
        "\n",
        "sentence = []\n",
        "for word in train_ds.text[1].split():\n",
        "    stemmer = PorterStemmer()\n",
        "    sentence.append(stemmer.stem(word))\n",
        "print(\"Stemming:\")\n",
        "print(' '.join(sentence))\n",
        "print()\n",
        "\n",
        "sentence = []\n",
        "for word in train_ds.text[1].split():\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    sentence.append(lemmatizer.lemmatize(word))\n",
        "print(\"Lemmatizing\")\n",
        "print(' '.join(sentence))\n",
        "print()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:24:21.970573Z",
          "iopub.execute_input": "2022-07-04T10:24:21.972208Z",
          "iopub.status.idle": "2022-07-04T10:24:24.637789Z",
          "shell.execute_reply.started": "2022-07-04T10:24:21.972162Z",
          "shell.execute_reply": "2022-07-04T10:24:24.634422Z"
        },
        "trusted": true,
        "id": "DSZjGb_qKaxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text pre-processing with stop-words and stemming\n",
        "analyzer = CountVectorizer().build_analyzer()\n",
        "# custom function for stemming and stop word reomval\n",
        "def stemming_words(doc):\n",
        "    stemmed_words = [stemmer.stem(w) for w in analyzer(doc)] # stemming the words\n",
        "    non_stop_words = [word for word in stemmed_words if word not in my_stop_words1] # keep non stop words only\n",
        "    return non_stop_words"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:24:24.639646Z",
          "iopub.execute_input": "2022-07-04T10:24:24.641942Z",
          "iopub.status.idle": "2022-07-04T10:24:24.65179Z",
          "shell.execute_reply.started": "2022-07-04T10:24:24.641866Z",
          "shell.execute_reply": "2022-07-04T10:24:24.64961Z"
        },
        "trusted": true,
        "id": "XC9kX7NgKaxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer2 = CountVectorizer(analyzer=stemming_words, max_features=1000)\n",
        "feature_vector2 = count_vectorizer2.fit(train_ds.text)\n",
        "train_ds_features2 = count_vectorizer2.transform(train_ds.text)\n",
        "features = feature_vector2.get_feature_names()\n",
        "features_counts2 = np.sum(train_ds_features2.toarray(), axis=0)\n",
        "feature_counts2 = pd.DataFrame(dict(features = features,\n",
        "                                    counts = features_counts2))\n",
        "feature_counts2.sort_values('counts', ascending=False)[:15]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:24:24.654731Z",
          "iopub.execute_input": "2022-07-04T10:24:24.655765Z",
          "iopub.status.idle": "2022-07-04T10:24:29.975387Z",
          "shell.execute_reply.started": "2022-07-04T10:24:24.655666Z",
          "shell.execute_reply": "2022-07-04T10:24:29.974229Z"
        },
        "trusted": true,
        "id": "ceTdrtXUKaxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crete final train data frame to build models\n",
        "trn_ds_df = pd.DataFrame(train_ds_features2.todense()) # convert sparse array to dataframe\n",
        "trn_ds_df.columns = features  # assign features of train daraframe\n",
        "trn_ds_df['sentiment'] = train_ds.sentiment\n",
        "trn_ds_df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:24:29.977029Z",
          "iopub.execute_input": "2022-07-04T10:24:29.977401Z",
          "iopub.status.idle": "2022-07-04T10:24:30.042766Z",
          "shell.execute_reply.started": "2022-07-04T10:24:29.977335Z",
          "shell.execute_reply": "2022-07-04T10:24:30.041644Z"
        },
        "trusted": true,
        "id": "qbuC5mmeKaxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trn_ds_df.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:24:30.044529Z",
          "iopub.execute_input": "2022-07-04T10:24:30.045455Z",
          "iopub.status.idle": "2022-07-04T10:24:30.058461Z",
          "shell.execute_reply.started": "2022-07-04T10:24:30.045406Z",
          "shell.execute_reply": "2022-07-04T10:24:30.056722Z"
        },
        "trusted": true,
        "id": "1TAZFOvQKaxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bar_df = feature_counts2.sort_values('counts', ascending=False)[:15]\n",
        "feature_list = list(bar_df.features)\n",
        "\n",
        "fig, ax = plt.subplots(5,3, figsize=(12,15))\n",
        "for i, j in enumerate(feature_list):\n",
        "    sns.barplot(x='sentiment', y=j ,data=trn_ds_df, estimator=sum, ax=ax[i//3, i%3])\n",
        "    ax[i//3, i%3].grid(visible=True, linestyle='--')\n",
        "    ax[i//3, i%3].set_title(f'for feature/word {j}')\n",
        "\n",
        "plt.suptitle(\"Sentiments of top most used words in reviews\", fontsize=17, fontweight='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:24:30.060979Z",
          "iopub.execute_input": "2022-07-04T10:24:30.06234Z",
          "iopub.status.idle": "2022-07-04T10:24:54.463359Z",
          "shell.execute_reply.started": "2022-07-04T10:24:30.062265Z",
          "shell.execute_reply": "2022-07-04T10:24:54.462147Z"
        },
        "trusted": true,
        "id": "M4i8JbB7Kaxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight:**\n",
        "\n",
        "**We can see word/feature appreance and its sentiment mostly negative words like 'hate', 'stupid' are having (0) negative sentiment and positive words 'love', 'like' are having positive sentiment.**\n",
        "\n",
        "a**lso by keeping most used words which most likely to map target variable will improve sentiment classification accuracy.**"
      ],
      "metadata": {
        "id": "DjCo_vhlKaxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wordcloud of reviews"
      ],
      "metadata": {
        "id": "pkYwjccTKaxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "wc = WordCloud(max_words=1000,\n",
        "               min_font_size=10,\n",
        "               height=600,\n",
        "               width=1600,\n",
        "               background_color='white',\n",
        "               stopwords=STOPWORDS).generate(' '.join(train_ds.text))\n",
        "\n",
        "plt.imshow(wc, interpolation= \"bilinear\")\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:24:54.464964Z",
          "iopub.execute_input": "2022-07-04T10:24:54.465485Z",
          "iopub.status.idle": "2022-07-04T10:24:57.120458Z",
          "shell.execute_reply.started": "2022-07-04T10:24:54.465435Z",
          "shell.execute_reply": "2022-07-04T10:24:57.118768Z"
        },
        "trusted": true,
        "id": "JwpOEyXXKaxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Naive-Bayes model for sentiment classification"
      ],
      "metadata": {
        "id": "ZYdpyVcuKaxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_X,test_X,train_y,test_y = train_test_split(train_ds_features2, train_ds.sentiment,\n",
        "                                                   test_size = 0.3, random_state=42)\n",
        "print(train_X.shape)\n",
        "print(test_X.shape)\n",
        "\n",
        "# naive-bayes classifier\n",
        "nb_clf = BernoulliNB()\n",
        "nb_clf.fit(train_X.toarray(), train_y)\n",
        "prediction = nb_clf.predict(test_X.toarray())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:24:57.123219Z",
          "iopub.execute_input": "2022-07-04T10:24:57.123813Z",
          "iopub.status.idle": "2022-07-04T10:24:57.318973Z",
          "shell.execute_reply.started": "2022-07-04T10:24:57.123764Z",
          "shell.execute_reply": "2022-07-04T10:24:57.311105Z"
        },
        "trusted": true,
        "id": "aHx6in3iKaxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classification report of test out output and prediction\n",
        "classificationreport = classification_report(test_y, prediction)\n",
        "print(classificationreport)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:24:57.327889Z",
          "iopub.execute_input": "2022-07-04T10:24:57.328463Z",
          "iopub.status.idle": "2022-07-04T10:24:57.380405Z",
          "shell.execute_reply.started": "2022-07-04T10:24:57.328413Z",
          "shell.execute_reply": "2022-07-04T10:24:57.378614Z"
        },
        "trusted": true,
        "id": "ay_whyFwKaxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the confusion matrix\n",
        "cm = confusion_matrix(test_y, prediction)\n",
        "sns.heatmap(cm, annot=True, fmt=\".2f\")\n",
        "plt.title(\"Confusion Matrix of NB classifier\")\n",
        "plt.ylabel(\"Actual values\")\n",
        "plt.xlabel(\"Predicted values\")\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:24:57.382515Z",
          "iopub.execute_input": "2022-07-04T10:24:57.383017Z",
          "iopub.status.idle": "2022-07-04T10:24:57.676307Z",
          "shell.execute_reply.started": "2022-07-04T10:24:57.38297Z",
          "shell.execute_reply": "2022-07-04T10:24:57.675442Z"
        },
        "trusted": true,
        "id": "aikyQ5riKaxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:**\n",
        "\n",
        "**Naive-Bayes model gives you an accuracy of 98% for sentiment classification model.**"
      ],
      "metadata": {
        "id": "d5k7bzJ0Kaxg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Using TF-IDF vectorizer"
      ],
      "metadata": {
        "id": "b_xMlEcBKaxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trn_X,tst_X,trn_y,tst_y = train_test_split(train_ds.text, train_ds.sentiment,\n",
        "                                                   test_size = 0.3, random_state=42)\n",
        "\n",
        "# transform dataset into cleaned vectorizors\n",
        "tfidf_vectorizer = TfidfVectorizer(analyzer= stemming_words,\n",
        "                                   max_features = 1000)\n",
        "feature_train = tfidf_vectorizer.fit_transform(trn_X)\n",
        "feature_test = tfidf_vectorizer.transform(tst_X)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:24:57.677604Z",
          "iopub.execute_input": "2022-07-04T10:24:57.678556Z",
          "iopub.status.idle": "2022-07-04T10:25:00.213764Z",
          "shell.execute_reply.started": "2022-07-04T10:24:57.678522Z",
          "shell.execute_reply": "2022-07-04T10:25:00.212942Z"
        },
        "trusted": true,
        "id": "5UpRfYa7Kaxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models to be trained\n",
        "models = [DecisionTreeClassifier(),\n",
        "          RandomForestClassifier(),\n",
        "          LogisticRegression(max_iter=1000),\n",
        "          KNeighborsClassifier(),\n",
        "          BernoulliNB()]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:25:00.215359Z",
          "iopub.execute_input": "2022-07-04T10:25:00.2165Z",
          "iopub.status.idle": "2022-07-04T10:25:00.222663Z",
          "shell.execute_reply.started": "2022-07-04T10:25:00.216454Z",
          "shell.execute_reply": "2022-07-04T10:25:00.22145Z"
        },
        "trusted": true,
        "id": "LKMYIST9Kaxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = []\n",
        "\n",
        "for model in models:\n",
        "    cross_val = cross_val_score(model, feature_train, trn_y, scoring='accuracy',\n",
        "                                cv= StratifiedKFold(10)).mean()\n",
        "    accuracy.append(cross_val)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:25:00.223999Z",
          "iopub.execute_input": "2022-07-04T10:25:00.224354Z",
          "iopub.status.idle": "2022-07-04T10:25:08.719275Z",
          "shell.execute_reply.started": "2022-07-04T10:25:00.224311Z",
          "shell.execute_reply": "2022-07-04T10:25:08.717684Z"
        },
        "trusted": true,
        "id": "g-l44IsUKaxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models_name = ['DecisionTreeClassifier', 'RandomForestClassifier',\n",
        "         'LogisticRegression', 'KNeighborsClassifier', 'BernoulliNB']\n",
        "\n",
        "accuracy_df = pd.DataFrame({'Model': models_name, 'Accuracy': accuracy})\n",
        "accuracy_df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:25:08.72033Z",
          "iopub.execute_input": "2022-07-04T10:25:08.720659Z",
          "iopub.status.idle": "2022-07-04T10:25:08.735874Z",
          "shell.execute_reply.started": "2022-07-04T10:25:08.72063Z",
          "shell.execute_reply": "2022-07-04T10:25:08.733942Z"
        },
        "trusted": true,
        "id": "6yIh5tVFKaxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression is best performer for our dataset so will build loistic regression model for sentiment classification.**"
      ],
      "metadata": {
        "id": "RIu9X3NcKaxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_reg = LogisticRegression(max_iter=1000)\n",
        "logistic_reg.fit(feature_train, trn_y)\n",
        "prediction_log = logistic_reg.predict(feature_test)\n",
        "clf_report = classification_report(tst_y, prediction_log)\n",
        "print(clf_report)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:25:08.737775Z",
          "iopub.execute_input": "2022-07-04T10:25:08.73813Z",
          "iopub.status.idle": "2022-07-04T10:25:08.777909Z",
          "shell.execute_reply.started": "2022-07-04T10:25:08.738099Z",
          "shell.execute_reply": "2022-07-04T10:25:08.777089Z"
        },
        "trusted": true,
        "id": "AmsfL8UyKaxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the confusion matrix\n",
        "cm_log = confusion_matrix(tst_y, prediction_log)\n",
        "sns.heatmap(cm_log, annot=True, fmt=\".2f\")\n",
        "plt.title(\"Confusion Matrix of logistics regression classifier\")\n",
        "plt.ylabel(\"Actual values\")\n",
        "plt.xlabel(\"Predicted values\")\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:25:08.779131Z",
          "iopub.execute_input": "2022-07-04T10:25:08.780059Z",
          "iopub.status.idle": "2022-07-04T10:25:09.048228Z",
          "shell.execute_reply.started": "2022-07-04T10:25:08.780021Z",
          "shell.execute_reply": "2022-07-04T10:25:09.047314Z"
        },
        "trusted": true,
        "id": "mUtxD9pRKaxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:**\n",
        "\n",
        "**With logistic regression we got around 99% accuracy which is more than Naive-Bayes classifier apparently.**"
      ],
      "metadata": {
        "id": "NfAKIHAvKaxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Using n-Grams range with TF-IDF vectorizor"
      ],
      "metadata": {
        "id": "1mBkrXT-Kaxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define get_stemmed_tokens for n-grams\n",
        "def get_stemmed_tokens(doc):\n",
        "    all_tokens = [word for word in nltk.word_tokenize(doc)]\n",
        "    clean_tokens = []\n",
        "    for each_token in all_tokens:\n",
        "        if re.search('[a-zA-Z]', each_token):\n",
        "            clean_tokens.append(each_token)\n",
        "    # stem the words\n",
        "    stemmed_tokens = [stemmer.stem(t) for t in clean_tokens]\n",
        "    return stemmed_tokens"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:25:09.049495Z",
          "iopub.execute_input": "2022-07-04T10:25:09.050853Z",
          "iopub.status.idle": "2022-07-04T10:25:09.058135Z",
          "shell.execute_reply.started": "2022-07-04T10:25:09.050814Z",
          "shell.execute_reply": "2022-07-04T10:25:09.056644Z"
        },
        "trusted": true,
        "id": "iNenZAbZKaxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vector = TfidfVectorizer(max_features=1000,\n",
        "                               stop_words='english',\n",
        "                               tokenizer = get_stemmed_tokens,\n",
        "                               ngram_range=(1,2))\n",
        "\n",
        "feature_trn = tfidf_vector.fit_transform(trn_X)\n",
        "feature_tst = tfidf_vector.transform(tst_X)\n",
        "# build Naive-bayes model\n",
        "NB_model = BernoulliNB()\n",
        "NB_model.fit(feature_trn, trn_y)\n",
        "preds = NB_model.predict(feature_tst)\n",
        "clf_report = classification_report(tst_y, preds)\n",
        "print(clf_report)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-04T10:25:09.059825Z",
          "iopub.execute_input": "2022-07-04T10:25:09.060246Z",
          "iopub.status.idle": "2022-07-04T10:25:13.220343Z",
          "shell.execute_reply.started": "2022-07-04T10:25:09.060203Z",
          "shell.execute_reply": "2022-07-04T10:25:13.219162Z"
        },
        "trusted": true,
        "id": "6U2SPGG2Kaxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:**\n",
        "\n",
        "**Using n-Grams range of (1,2) we received 98% accuracy of sentiment classification using Naive-Bayes model.**"
      ],
      "metadata": {
        "id": "RLuEeV_gKaxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion:\n",
        "\n",
        "**1. We build 5 different models using Count vectorization and TF-IDF vectorization techniques to extract features from text data**\n",
        "\n",
        "**2. We used stop-words removal, stemming and regular expressions for text data cleaning**\n",
        "\n",
        "**3. With Logistic regression model we recieved highest accuracy of 99% and with Naive-Bayes model we recieved accuracy of 98%.**"
      ],
      "metadata": {
        "id": "sVpEaWgCKaxi"
      }
    }
  ]
}